<p>
  Write a GPU program that interleaves two arrays of 32-bit floating point numbers.
  Given two input arrays <code>A</code> and <code>B</code>, each of length <code>N</code>,
  produce an output array of length <code>2N</code> where elements alternate between the two inputs:
  <code>[A[0], B[0], A[1], B[1], A[2], B[2], ...]</code>
</p>

<h2>Implementation Requirements</h2>
<ul>
  <li>Use only native features (external libraries are not permitted)</li>
  <li>The <code>solve</code> function signature must remain unchanged</li>
  <li>The final result must be stored in the <code>output</code> array</li>
</ul>

<h2>Example 1:</h2>
<pre>
Input:  A = [1.0, 2.0, 3.0], B = [4.0, 5.0, 6.0]
Output: [1.0, 4.0, 2.0, 5.0, 3.0, 6.0]
</pre>

<h2>Example 2:</h2>
<pre>
Input:  A = [10.0, 20.0], B = [30.0, 40.0]
Output: [10.0, 30.0, 20.0, 40.0]
</pre>

<h2>Constraints</h2>
<ul>
  <li>1 &le; <code>N</code> &le; 50,000,000</li>
</ul>
