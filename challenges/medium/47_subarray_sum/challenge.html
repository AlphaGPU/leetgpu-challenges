<p>
  Implement a program that performs element-wise addition of two vectors containing 32-bit floating point numbers on a GPU. 
  You are given an input array <code>input</code> of length <code>N</code>, and two indices <code>S</code> and <code>E</code>.
  <code>S</code> and <code>E</code> are inclusive, 0-based start and end indices â€” compute the sum of <code>input[S..E]</code>.
</p>

<h2>Implementation Requirements</h2>
<ul>
  <li>Use only native features (external libraries are not permitted)</li>
  <li>The <code>solve</code> function signature must remain unchanged</li>
  <li>The final result must be stored in the <code>output</code> variable</li>
</ul>

<h2>Example 1:</h2>
<pre>
Input: input = [1, 2, 1, 3, 4], S = 1, E = 3
Output: output = 6
</pre>

<h2>Example 2:</h2>
<pre>
Input: input = [1, 2, 3, 4], S = 0, E = 3
Output: output = 10
</pre>

<h2>Constraints</h2>
<ul>
  <li>1 &le; <code>N</code> &le; 100,000,000</li>
  <li>1 &le; <code>input[i]</code> &le; 10</li>
  <li>0 &le; <code>S</code> &le; <code>E</code> &le; <code>N - 1</code></li>
</ul>
