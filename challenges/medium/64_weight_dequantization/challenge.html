<p>
    In modern Large Language Model (LLM) inference, weights are often quantized to lower precision (like INT8 or FP8) to save memory and increase bandwidth.
    A common technique is <strong>block-wise quantization</strong>, where a large weight matrix is divided into smaller blocks (e.g., 128x128), and each block shares a single scaling factor.
</p>
<p>
    Implement a kernel that "dequantizes" a weight matrix. You are given an input matrix \(X\) of shape \((M, N)\) containing quantized values and a scale matrix \(S\) of shape \((\lceil M/B \rceil, \lceil N/B \rceil)\), where \(B\) is the block size.
</p>
<p>
    For each element \(X_{ij}\), the corresponding scale factor is \(S_{row, col}\) where \(row = i / B\) and \(col = j / B\).
    The output \(Y_{ij}\) should be computed as:
    \[
    Y_{ij} = X_{ij} \times S_{row, col}
    \]
</p>
<h2>Implementation Requirements</h2>
<ul>
    <li>Implement the kernel efficiently using Triton</li>
    <li>Handle arbitrary \(M\) and \(N\) dimensions (padding might be needed implicitly via masking)</li>
    <li>The block size <code>BLOCK_SIZE</code> is a compile-time constant (constexpr)</li>
</ul>
<h2>Example 1:</h2>
<pre>Input:  M, N = 256, 256
        BLOCK_SIZE = 128
        X = random((256, 256))
        S = matrix of shape (2, 2)

Output: Block (0,0) of X (top-left 128x128) is multiplied by S[0,0]
        Block (0,1) of X (top-right 128x128) is multiplied by S[0,1]
        ... and so on.</pre>